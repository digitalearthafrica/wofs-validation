{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Institution data to AEZs and Data cleaning\n",
    "\n",
    "The validation data is orignally labelled by institution, this notebook will clip insitutionally labelled data to agro-ecological regions of Africa. After spatially clipping the data, the data is cleaned: duplicates are removed, observations that were cloudy or returned an 'NA' values from the datacube query, or were mislabelled are removed.\n",
    "\n",
    "**Input data** : `<INSTITUTION_NAME>_wofs_ls_valid.csv>`\n",
    "\n",
    "**Output_data** : `<AEZ>_wofs_ls_validation_points.csv`\n",
    "\n",
    "Last modified: 13/02/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.10.3-CAPI-1.16.1). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../02_Validation_results/WOfS_Assessment/wofs_ls/Institutions/'\n",
    "\n",
    "AGRYHMET = pd.read_csv(path+'AGRHYMET_wofs_ls_valid.csv')\n",
    "RCMRD = pd.read_csv(path+'RCMRD_wofs_ls_valid.csv')\n",
    "OSS = pd.read_csv(path+'/OSS_wofs_ls_valid.csv')\n",
    "AFRIGIST = pd.read_csv(path+'AFRIGIST_wofs_ls_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 40131\n"
     ]
    }
   ],
   "source": [
    "# concatenate the institution data into a big table and check total length\n",
    "combined = pd.concat([AGRYHMET, RCMRD, OSS, AFRIGIST]).reset_index(drop=True).drop('Unnamed: 0', axis=1)\n",
    "print('Total Number of points:', len(combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that they are all combined into one table, we need to turn it into geopandas to be able to compare it with the shape files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a geopandas object\n",
    "geo_combined = gpd.GeoDataFrame(combined, geometry=gpd.points_from_xy(combined.LON, combined.LAT), crs='EPSG:4326')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip points to AEZ's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../02_Validation_data/AEZ_shapefiles/'\n",
    "east = gpd.read_file(path+'Eastern.shp')\n",
    "west = gpd.read_file(path+'Western.shp')\n",
    "north = gpd.read_file(path+'Northern.shp')\n",
    "south = gpd.read_file(path+'Southern.shp')\n",
    "sahel = gpd.read_file(path+'Sahel.shp')\n",
    "central = gpd.read_file(path+'Central.shp')\n",
    "io = gpd.read_file(path+'Indian_ocean.shp')\n",
    "\n",
    "shapes = [east,west,north,south,sahel,central,io]\n",
    "aezs= ['Eastern', 'Western', 'Northern', 'Southern', 'Sahel', 'Central', 'Indian_ocean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through AEZs and clip points to region, clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eastern Before cleaning: 6093\n",
      "   No. of duplicates: 243\n",
      "   No. of cloudy obs: 3248\n",
      "   No. of NaNs: 28\n",
      "   Eastern After cleaning: 2574\n",
      "\n",
      "\n",
      "   No. of unique locations: 461\n",
      "\n",
      "\n",
      "Western Before cleaning: 8038\n",
      "   No. of duplicates: 4003\n",
      "   No. of cloudy obs: 2894\n",
      "   No. of NaNs: 28\n",
      "   Western After cleaning: 1113\n",
      "\n",
      "\n",
      "   No. of unique locations: 402\n",
      "\n",
      "\n",
      "Northern Before cleaning: 3597\n",
      "   No. of duplicates: 4\n",
      "   No. of cloudy obs: 2409\n",
      "   No. of NaNs: 4\n",
      "   Northern After cleaning: 1180\n",
      "\n",
      "\n",
      "   No. of unique locations: 229\n",
      "\n",
      "\n",
      "Southern Before cleaning: 7216\n",
      "   No. of duplicates: 2368\n",
      "   No. of cloudy obs: 2653\n",
      "   No. of NaNs: 113\n",
      "   Southern After cleaning: 2082\n",
      "\n",
      "\n",
      "   No. of unique locations: 410\n",
      "\n",
      "\n",
      "Sahel Before cleaning: 3577\n",
      "   No. of duplicates: 18\n",
      "   No. of cloudy obs: 2314\n",
      "   No. of NaNs: 9\n",
      "   Sahel After cleaning: 1236\n",
      "\n",
      "\n",
      "   No. of unique locations: 255\n",
      "\n",
      "\n",
      "Central Before cleaning: 7937\n",
      "   No. of duplicates: 3732\n",
      "   No. of cloudy obs: 3068\n",
      "   No. of NaNs: 41\n",
      "   Central After cleaning: 1096\n",
      "\n",
      "\n",
      "   No. of unique locations: 343\n",
      "\n",
      "\n",
      "Indian_ocean Before cleaning: 3673\n",
      "   No. of duplicates: 158\n",
      "   No. of cloudy obs: 1256\n",
      "   No. of NaNs: 177\n",
      "   Indian_ocean After cleaning: 2082\n",
      "\n",
      "\n",
      "   No. of unique locations: 277\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total_before=[]\n",
    "total_after=[]\n",
    "total_sites_before=[]\n",
    "total_sites_after=[]\n",
    "for s, a in zip(shapes, aezs):\n",
    "    \n",
    "    #clip data to AEZ boundary\n",
    "    geo_combined = geo_combined.rename(columns={\"WATERFLAG\": \"ACTUAL\"})\n",
    "    gdf = gpd.overlay(geo_combined, s, how='intersection')\n",
    "    \n",
    "    #tally number of samples\n",
    "    raw_n = len(gdf)\n",
    "    print(a,'Before cleaning:', raw_n)\n",
    "    total_before.append(raw_n)\n",
    "    #tally number of unique locations\n",
    "    total_sites_before.append(len((gdf.PLOT_ID).unique()))\n",
    "        \n",
    "    # setting the class_wet column to be prediction\n",
    "    gdf[\"PREDICTION\"] = gdf[\"CLASS_WET\"].apply(lambda x: \"1\" if x >= 1 else \"0\")\n",
    "    \n",
    "    # Remove the duplicated plot IDs which means those that are labeled for similar month as 0, 1, 2  or 3.\n",
    "    # find all duplicated entries\n",
    "    dup = gdf[gdf.duplicated([\"PLOT_ID\", \"MONTH\"], keep=False)]\n",
    "    while len(dup)>0:\n",
    "        # keep actual duplicated entries (if duplicated entries have the same actual values, then they are still valid points to use)\n",
    "        dup = dup.drop_duplicates([\"PLOT_ID\", \"MONTH\", \"ACTUAL\"])\n",
    "        # drop duplicated rows\n",
    "        gdf = gdf.drop(dup.index)\n",
    "        dup = gdf[gdf.duplicated([\"PLOT_ID\", \"MONTH\"], keep=False)]\n",
    "    \n",
    "    # check dups agains\n",
    "    if len(gdf[gdf.duplicated([\"PLOT_ID\", \"MONTH\"], keep=False)])>0:\n",
    "        print(\"dups?\")\n",
    "        break\n",
    "    \n",
    "    #tally number of duplicates removed\n",
    "    dup = raw_n - len(gdf)\n",
    "    print('   No. of duplicates:' , dup)\n",
    "    \n",
    "    # Filter out those rows that are labeled more than 1 or there is no clear WOfS/SCL observations\n",
    "    not_clear = gdf[(gdf[\"ACTUAL\"] > 1) | (gdf[\"CLEAR_OBS\"] == 0.0)].index\n",
    "    gdf = gdf.drop(not_clear)\n",
    "    \n",
    "    nans = gdf[gdf[\"CLEAR_OBS\"].isna()].index\n",
    "    gdf = gdf.drop(nans)\n",
    "    \n",
    "    #tally number of cloudy obs\n",
    "    print('   No. of cloudy obs:' , len(not_clear))\n",
    "    print('   No. of NaNs:' , len(nans))\n",
    "    \n",
    "    #tally samples left after cleaning\n",
    "    print('  ',a,'After cleaning:', len(gdf))\n",
    "    print('\\n')\n",
    "    total_after.append(len(gdf))\n",
    "    \n",
    "    #tally number of unique validation locations\n",
    "    print('   No. of unique locations:' , len((gdf.PLOT_ID).unique()))\n",
    "    print('\\n')\n",
    "    total_sites_after.append(len((gdf.PLOT_ID).unique()))\n",
    "\n",
    "    # save out to file for the accuracy assesments\n",
    "    gdf.to_csv('../02_Validation_results/WOfS_Assessment/wofs_ls/'+a+'_wofs_ls_validation_points.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of data points before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples before cleaning:  40131\n",
      "Total samples after cleaning:  11363\n",
      "Total samples sites before cleaning:  2900\n",
      "Total samples sites after cleaning:  2377\n"
     ]
    }
   ],
   "source": [
    "print('Total samples before cleaning: ', sum(total_before))\n",
    "print('Total samples after cleaning: ', sum(total_after))\n",
    "print('Total samples sites before cleaning: ', sum(total_sites_before))\n",
    "print('Total samples sites after cleaning: ', sum(total_sites_after))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
