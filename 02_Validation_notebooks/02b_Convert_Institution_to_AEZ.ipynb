{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Institution data to AEZs and Data cleaning\n",
    "\n",
    "The validation data is orignally labelled by institution, this notebook will clip insitutionally labelled data to agro-ecological regions of Africa. After spatially clipping the data, the data is cleaned: duplicates are removed, observations that were cloudy or returned an 'NA' values from the datacube query, or were mislabelled are removed.\n",
    "\n",
    "**Input data** : `<INSTITUTION_NAME>_wofs_ls_valid.csv>`\n",
    "\n",
    "**Output_data** : `<AEZ>_wofs_ls_validation_points.csv`\n",
    "\n",
    "Last modified: 03/02/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the institution files, preferably the ones that have had columns dropped already in the processing step\n",
    "AGRYHMET = pd.read_csv('../02_Validation_results/WOfS_Assessment/wofs_ls/Institutions/AGRYHMET_wofs_ls_valid.csv')\n",
    "RCMRD = pd.read_csv('../02_Validation_results/WOfS_Assessment/wofs_ls/Institutions/RCMRD_wofs_ls_valid.csv')\n",
    "OSS = pd.read_csv('../02_Validation_results/WOfS_Assessment/wofs_ls/Institutions/OSS_wofs_ls_valid.csv')\n",
    "AFRIGIST = pd.read_csv('../02_Validation_results/WOfS_Assessment/wofs_ls/Institutions/AFRIGIST_wofs_ls_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of points: 40131\n"
     ]
    }
   ],
   "source": [
    "# concatenate the institution data into a big table and check total length\n",
    "combined = pd.concat([AGRYHMET, RCMRD, OSS, AFRIGIST]).reset_index(drop=True).drop('Unnamed: 0', axis=1)\n",
    "print('Total Number of points:', len(combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that they are all combined into one table, we need to turn it into geopandas to be able to compare it with the shape files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a geopandas object\n",
    "geo_combined = gpd.GeoDataFrame(combined, geometry=gpd.points_from_xy(combined.LON, combined.LAT), crs='EPSG:4326')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip points to AEZ's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "east = gpd.read_file('../02_Validation_data/AEZ_shapefiles/Eastern.shp')\n",
    "west = gpd.read_file('../02_Validation_data/AEZ_shapefiles/Western.shp')\n",
    "north = gpd.read_file('../02_Validation_data/AEZ_shapefiles/Northern.shp')\n",
    "south = gpd.read_file('../02_Validation_data/AEZ_shapefiles/Southern.shp')\n",
    "sahel = gpd.read_file('../02_Validation_data/AEZ_shapefiles/Sahel.shp')\n",
    "central = gpd.read_file('../02_Validation_data/AEZ_shapefiles/Central.shp')\n",
    "io = gpd.read_file('../02_Validation_data/AEZ_shapefiles/Indian_ocean.shp')\n",
    "\n",
    "shapes = [east,west,north,south,sahel,central,io]\n",
    "aezs= ['Eastern', 'Western', 'Northern', 'Southern', 'Sahel', 'Central', 'Indian_ocean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through AEZs and clip points to region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eastern Before cleaning: 6093\n",
      "   n duplicates: 243\n",
      "   n cloudy obs: 3181\n",
      "   Eastern After cleaning: 2669\n",
      "Western Before cleaning: 8038\n",
      "   n duplicates: 4021\n",
      "   n cloudy obs: 2962\n",
      "   Western After cleaning: 1055\n",
      "Northern Before cleaning: 3597\n",
      "   n duplicates: 4\n",
      "   n cloudy obs: 2468\n",
      "   Northern After cleaning: 1125\n",
      "Southern Before cleaning: 7216\n",
      "   n duplicates: 2377\n",
      "   n cloudy obs: 2684\n",
      "   Southern After cleaning: 2155\n",
      "Sahel Before cleaning: 3577\n",
      "   n duplicates: 18\n",
      "   n cloudy obs: 2335\n",
      "   Sahel After cleaning: 1224\n",
      "Central Before cleaning: 7937\n",
      "   n duplicates: 6600\n",
      "   n cloudy obs: 731\n",
      "   Central After cleaning: 606\n",
      "Indian_ocean Before cleaning: 3673\n",
      "   n duplicates: 158\n",
      "   n cloudy obs: 1232\n",
      "   Indian_ocean After cleaning: 2283\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total_before=[]\n",
    "total_after=[]\n",
    "for s, a in zip(shapes, aezs):\n",
    "    \n",
    "    #clip data to AEZ boundary\n",
    "    geo_combined = geo_combined.rename(columns={\"WATERFLAG\": \"ACTUAL\"})\n",
    "    gdf = gpd.overlay(geo_combined, s, how='intersection')\n",
    "    \n",
    "    #tally number of samples\n",
    "    raw_n = len(gdf)\n",
    "    print(a,'Before cleaning:', raw_n)\n",
    "    total_before.append(raw_n)\n",
    "    \n",
    "    # setting the class_wet column to be prediction\n",
    "    gdf[\"PREDICTION\"] = gdf[\"CLASS_WET\"].apply(lambda x: \"1\" if x >= 1 else \"0\")\n",
    "    \n",
    "    # Remove the duplicated plot IDs which means those that are labeled for similar month as 0, 1, 2  or 3.\n",
    "    gdf = gdf.drop_duplicates([\"LAT\", \"LON\", \"MONTH\"], keep=False)\n",
    "    \n",
    "    #tally number of duplicates removed\n",
    "    dup = raw_n - len(gdf)\n",
    "    print('   n duplicates:' , dup)\n",
    "    \n",
    "    # Filter out those rows that are labeled more than 1 or there is no clear WOfS/SCL observations\n",
    "    indexNames = gdf[\n",
    "        (gdf[\"ACTUAL\"] > 1) | (gdf[\"CLEAR_OBS\"] == 0.0) | (gdf[\"CLEAR_OBS\"].isna())\n",
    "    ].index\n",
    "    gdf = gdf.drop(indexNames)\n",
    "    \n",
    "    #tally number of cloudy obs\n",
    "    not_clear = (raw_n-dup) - len(gdf)\n",
    "    print('   n cloudy obs:' , not_clear)\n",
    "    \n",
    "    #tally samples left after cleaning\n",
    "    print('  ',a,'After cleaning:', len(gdf))\n",
    "    total_after.append(len(gdf))\n",
    "    \n",
    "    # save out to file for the accuracy assesments\n",
    "    gdf.to_csv('../02_Validation_results/WOfS_Assessment/wofs_ls/'+a+'_wofs_ls_validation_points.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of data points before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples before cleaning:  40131\n",
      "Total samples after cleaning:  11117\n"
     ]
    }
   ],
   "source": [
    "print('Total samples before cleaning: ', sum(total_before))\n",
    "print('Total samples after cleaning: ', sum(total_after))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
