{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified random sampling from NDWI mosaic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_spatialtools import xr_rasterize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define area name\n",
    "\n",
    "area_name = 'Southern'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to one mosaic (only do it once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tif\n",
    "\n",
    "if not os.path.exists(f\"NDWI_composite/{area_name.lower()}_NDWI_mosaic.tif\"):\n",
    "    os.chdir('NDWI_composite')\n",
    "    os.system(f\"gdalbuildvrt {area_name.lower()}_NDWI_mosaic.vrt {area_name.lower()}_NDWI_tile*.tif\")\n",
    "    os.system(\"gdal_translate \"\\\n",
    "       \"-co BIGTIFF=YES \"\\\n",
    "       \"-co COMPRESS=DEFLATE \"\\\n",
    "       \"-co ZLEVEL=9 \"\\\n",
    "       \"-co PREDICTOR=1 \"\\\n",
    "       \"-co TILED=YES \"\\\n",
    "       \"-co BLOCKXSIZE=1024 \"\\\n",
    "       \"-co BLOCKYSIZE=1024 \"\\\n",
    "       +f\"{area_name.lower()}_NDWI_mosaic.vrt \"+ f\"{area_name.lower()}_NDWI_mosaic.tif\")\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NDWI mosaic and clip to AEZ (TODO: use AEZ-large_water_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(area_name): os.mkdir(area_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_rasterio(f\"NDWI_composite/{area_name.lower()}_NDWI_mosaic.tif\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/pyproj/crs/crs.py:280: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  projstring = _prepare_from_string(projparams)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterizing to match xarray.DataArray dimensions (104514, 93681) and projection system/CRS (e.g. +init=epsg:6933)\n"
     ]
    }
   ],
   "source": [
    "#load shapefile\n",
    "#gdf = gpd.read_file(f'../../shapes/simplified_AEZs/{area_name}.shp')\n",
    "gdf = gpd.read_file(f'../../shapes/AEZs_ExcludeLargeWB/AEZs_ExcludeLargeWB_update_{area_name}.shp')\n",
    "\n",
    "#rasterize shapeile\n",
    "mask = xr_rasterize(gdf=gdf,\n",
    "                     da=ds)\n",
    "\n",
    "ds = ds.where(mask)\n",
    "ds = ds.where(ds!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.to_dataset(name='ndwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ds.plot.imshow();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check NDWI distribution and determine thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 bins\n",
    "freq_thresh = [0.1, 0.3, 0.6, 0.9]\n",
    "n_class = len(freq_thresh)+1\n",
    "frac_sample = [0.1, 0.1, 0.2, 0.3, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{area_name}/ndwi_{area_name}.csv'):\n",
    "    histy, histx, tmp = dataset.ndwi.plot.hist(bins=100, cumulative=True, density=True);\n",
    "    np.savetxt(f'{area_name}/ndwi_{area_name}.csv', np.vstack((histx[1:], histy)).transpose(),fmt='%.3f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use wofs\n",
    "x, y, t = np.loadtxt(f'wofs_summary_aez/wofs_{area_name}.csv', delimiter=',', unpack=True)\n",
    "perc = np.interp(freq_thresh, x, y)\n",
    "print('percentile for ephemeral and permanent water', perc)\n",
    "histx, histy = np.loadtxt(f'{area_name}/ndwi_{area_name}.csv', delimiter=',', unpack=True)\n",
    "thresh = np.interp(perc, histy, histx)\n",
    "print('Thresholds', thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Southern Thresholds [-0.071488 -0.048836 -0.000376  0.033232]\n",
    "* Eastern Thresholds [-0.0627   -0.043991 -0.035081  0.030288]\n",
    "* Central Thresholds [-0.108416 -0.083291 -0.048377 -0.013617]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify into bins of different water detection frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less\n",
      "  \n",
      "/env/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  \n",
      "/env/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in less\n",
      "  \n",
      "/env/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# fix thresholds\n",
    "thresh = [-0.07, -0.05, -0.03, 0.03]\n",
    "\n",
    "label = np.zeros_like(dataset.ndwi.values, dtype=np.uint8)\n",
    "\n",
    "label +=(dataset.ndwi.values<thresh[0]).astype(np.uint8)*1\n",
    "for i in range(2, n_class):\n",
    "    label += ((dataset.ndwi.values>=thresh[i-2]) & (dataset.ndwi.values<thresh[i-1])).astype(np.uint8)*i\n",
    "\n",
    "label += (dataset.ndwi.values>=thresh[-1]).astype(np.uint8)*n_class\n",
    "\n",
    "dataset['label'] = ('y','x'), label\n",
    "dataset['label'].attrs = dataset.ndwi.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('Southern/Southern_label.tif')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save classes\n",
    "\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "write_cog(dataset.label, f'{area_name}/{area_name}_label.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the labels are alreay saved, read the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "data = xr.open_rasterio(f'{area_name}/{area_name}_label.tif').squeeze()\n",
    "dataset = data.to_dataset(name='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample from array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take a while, and we already know only class 1 (dry) is dominant\n",
    "\n",
    "#class_sizes =[]\n",
    "#for class_id in np.arange(1, n_class+1):\n",
    "#    class_sizes.append((dataset.label==class_id).sum().values)\n",
    "\n",
    "#class_sizes = np.array(class_sizes)\n",
    "#print(class_sizes)\n",
    "#print(class_sizes/class_sizes.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50  50 100 150 150]\n"
     ]
    }
   ],
   "source": [
    "if area_name in ['Western', 'Eastern', 'Southern', 'Central']: \n",
    "    n_sample = 500\n",
    "else: n_sample = 300\n",
    "\n",
    "# distribute points across classes\n",
    "n_sample_class = (n_sample * np.array(frac_sample)).astype(int) #np.ceil(n_sample*1./ n_class).astype(int)\n",
    "print(n_sample_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_common(labelarray, label, n_sample, min_dist=0, return_index=True):\n",
    "    \"\"\"\n",
    "    Pick random samples from a prevalent class\n",
    "    \n",
    "    \"\"\"\n",
    "    picked_y = np.array([], dtype=int)\n",
    "    picked_x = np.array([], dtype=int)\n",
    "    while len(picked_y) < n_sample:\n",
    "        n_to_pick = n_sample - len(picked_y)\n",
    "        # over sample without knowing total number of points in this class\n",
    "        n_sample_over = 5*n_to_pick\n",
    "        random_x = np.random.choice(np.arange(len(labelarray.x)), n_sample_over, replace=False)\n",
    "        random_y = np.random.choice(np.arange(len(labelarray.y)), n_sample_over, replace=False)\n",
    "        # keep points matching label\n",
    "        match = labelarray.values[random_y, random_x] == label\n",
    "        random_y, random_x = random_y[match], random_x[match]\n",
    "        # remove points too close to previously picked ones\n",
    "        if len(picked_y) > 0 and min_dist > 0:\n",
    "            dist = np.sqrt((random_y-picked_y[:, None])**2 + (random_x-picked_x[:, None])**2)\n",
    "            keep = dist.min(axis=0) >= min_dist\n",
    "            random_y, random_x = random_y[keep], random_x[keep]\n",
    "        # remove points too close to others\n",
    "        if min_dist > 0:\n",
    "            dist = np.sqrt((random_y-random_y[:, None])**2 + (random_x-random_x[:, None])**2)\n",
    "            # set distances to themselves to min_dist\n",
    "            dist[np.arange(len(random_y)), np.arange(len(random_x))] = min_dist\n",
    "            keep = dist.min(axis=0) >= min_dist\n",
    "            random_y, random_x = random_y[keep], random_x[keep]\n",
    "        # remove extra points\n",
    "        if len(random_y) > n_to_pick:\n",
    "            pick = np.random.choice(np.arange(len(random_y)), n_to_pick, replace=False)\n",
    "            random_y, random_x = random_y[pick], random_x[pick]\n",
    "        picked_y, picked_x = np.concatenate((picked_y, random_y)), np.concatenate((picked_x, random_x))\n",
    "    \n",
    "    if return_index: return np.array(picked_y), np.array(picked_x)\n",
    "    else: return labelarray.y[np.array(picked_y)].values, labelarray.x[np.array(picked_x)].values\n",
    "\n",
    "\n",
    "def pick_random_rare(labelarray, label, n_sample, min_dist=0, return_index=True, n_points_per_batch=1):\n",
    "    \"\"\"\n",
    "    Pick random samples from a rare class\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # points will be picked from flattened index arrays\n",
    "    da_shape = labalarray.values.shape\n",
    "    index = np.argwhere(labelarray.values.flatten() == label).squeeze()\n",
    "    index_y, index_x = np.unravel_index(index, da_shape)\n",
    "    \n",
    "    picked_y = []\n",
    "    picked_x = []    \n",
    "    # pick one or a few points at a time\n",
    "    while len(picked_y) < n_sample:\n",
    "        picked = np.random.choice(index, n_points_per_batch, replace=False)\n",
    "        # convert back to x, y \n",
    "        random_y, random_x  = np.unravel_index(picked, da_shape)\n",
    "        if n_points_per_batch > 1 and min_dist > 0:\n",
    "            # remove points too close to others\n",
    "            dist = np.sqrt((random_y-random_y[:, None])**2 + (random_x-random_x[:, None])**2)\n",
    "            dist[np.arange(len(random_y)),np.arange(len(random_x))] = min_dist\n",
    "            keep = dist.min(axis=0) >= min_dist\n",
    "            random_y, random_x = random_y[keep], random_x[keep]\n",
    "        if min_dist > 0:\n",
    "            # remove nearby points in the index array\n",
    "            keep = np.sqrt((index_y-random_y[:, None])**2 + (index_x-random_x[:, None])**2).min(axis=0) >= min_dist\n",
    "            index, index_y, index_x = index[keep], index_y[keep], index_x[keep]\n",
    "        picked_y, picked_x = np.concatenate((picked_y, random_y)), np.concatenate((picked_x, random_x))\n",
    "        \n",
    "    if len(picked_y) > n_sample:\n",
    "        pick = np.random.choice(np.arange(len(picked_y)), n_sample, replace=False)\n",
    "        picked_y, picked_x = picked_y[pick], picked_x[pick]\n",
    "    \n",
    "    if return_index: return np.array(picked_y), np.array(picked_x)\n",
    "    else: return labelarray.y[np.array(picked_y)].values, labelarray.x[np.array(picked_x)].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 32.4 s, total: 2min 9s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from skimage.morphology import disk\n",
    "\n",
    "min_dist_between_class = True\n",
    "\n",
    "\n",
    "min_dist = 1000 # this is x, y index, so 30 km \n",
    "\n",
    "if min_dist_between_class:\n",
    "    # buffer around picked points\n",
    "    offset_y, offset_x = np.where(disk(min_dist)==1)\n",
    "    offset_y, offset_x = offset_y-min_dist, offset_x-min_dist\n",
    "\n",
    "    labelarray = dataset.label.copy()\n",
    "    da_shape = labelarray.values.shape\n",
    "else:\n",
    "    labelarray = dataset.label\n",
    "\n",
    "\n",
    "label_picked = {}\n",
    "\n",
    "class_id = 1\n",
    "y, x = pick_random_common(labelarray, class_id, n_sample_class[class_id-1], min_dist=min_dist, return_index=True)\n",
    "label_picked[class_id] = (y, x)\n",
    "\n",
    "# clear out areas next to picked points\n",
    "if min_dist_between_class:\n",
    "    for yx in zip(y,x):\n",
    "        buffer_y, buffer_x = yx[0]+offset_y, yx[1]+offset_x\n",
    "        # within boundary\n",
    "        mask_ind = (buffer_y>=0) & (buffer_x>=0) & (buffer_y<da_shape[0]) & (buffer_x<da_shape[1])\n",
    "        labelarray.values[buffer_y[mask_ind], buffer_x[mask_ind]] = 0\n",
    "    \n",
    "for class_id in np.arange(2, n_class+1):\n",
    "    \n",
    "    y, x = pick_random_rare(labelarray, class_id, n_sample_class[class_id-1], min_dist=min_dist, return_index=True,\n",
    "                            n_points_per_batch=10)\n",
    "    label_picked[class_id] = (y, x)\n",
    "\n",
    "    # clear out areas next to picked points\n",
    "    if min_dist_between_class:\n",
    "        for yx in zip(y, x):\n",
    "            buffer_y, buffer_x = yx[0]+offset_y, yx[1]+offset_x\n",
    "            # within boundary\n",
    "            mask_ind = (buffer_y>=0) & (buffer_x>=0) & (buffer_y<da_shape[0]) & (buffer_x<da_shape[1])\n",
    "            labelarray.values[buffer_y[mask_ind], buffer_x[mask_ind]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for class_id in np.arange(1, n_class+1):\n",
    "    y, x = label_picked[class_id]\n",
    "    df = pd.DataFrame({'y': dataset.y[y].values, 'x':dataset.x[x].values})\n",
    "    #df = pd.read_csv(f'{area_name}/{area_name}_class_{class_id}.csv', header=None, names=['y','x'] )\n",
    "    df['class']=class_id\n",
    "    if class_id ==1: \n",
    "        dfs = df\n",
    "    else: \n",
    "        dfs = dfs.append(df, ignore_index=True)\n",
    "\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "        dfs,\n",
    "        crs=dataset.label.crs,\n",
    "        geometry=gpd.points_from_xy(dfs.x, dfs.y)).reset_index()\n",
    "\n",
    "gdf = gdf.drop(['x', 'y'],axis=1)\n",
    "\n",
    "if min_dist_between_class:\n",
    "    gdf.to_file(f'{area_name}/{area_name}_samples_min_dist_between_class.shp')\n",
    "else:\n",
    "    gdf.to_file(f'{area_name}/{area_name}_samples.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
